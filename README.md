# Typhoon Instruction tuning with WikiThai-V03 dataset

## Motivation
As finetuning technique come to popular in the way to make many useful downstream applications, For low-resource Language LLMs, Thai LLMs is develop, at the same time good quality dataset is released, I have the idea to expleriment how improvement of LLMs for Q-A tasks after finetuning with the dataset

## Dataset for finetuning


## Fine-tuning process

## Finetuning Results 

## Analysis & further study for improving 

- **Clean Dataset**
- **Overall finetuning technique**
    - Loss curve from Training set monitor from Weight and Bias(WandB)
      
- **Inference technique**

## Resource

**For Finetuning tutorial**

- https://medium.com/@codersama/fine-tuning-mistral-7b-in-google-colab-with-qlora-complete-guide-60e12d437cca
- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb

**For inferencing model**
- https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge?utm_source=%2Fsearch%2Fqlora&utm_medium=reader2

**For original dataset**
- https://huggingface.co/datasets/pythainlp/thai-wiki-dataset-v3
